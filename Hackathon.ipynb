{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "def get_filtered_data(rootDir):\n",
    "    ''' Return filtered data from root folder as array of strings and \n",
    "    the corresponding paths to the respective files\n",
    "    '''\n",
    "    allDirs = []\n",
    "    allFiles = []\n",
    "    files_txt = []\n",
    "    data = []\n",
    "    dataFiltered = []\n",
    "    filesFiltered = []\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for root, dirs, files in os.walk(rootDir, topdown=False):\n",
    "        for name in dirs:\n",
    "            allDirs.append(os.path.join(root, name))\n",
    "\n",
    "    for dP in allDirs:\n",
    "        onlyFiles = [f for f in os.listdir(dP) if os.path.isfile(os.path.join(dP, f))]\n",
    "        for oF in onlyFiles: allFiles.append(os.path.join(dP, oF))\n",
    "\n",
    "    for aF in allFiles:\n",
    "        #with open(aF, 'r', encoding = \"ISO-8859-1\") as newFile: \n",
    "        with open(aF, 'r', encoding = \"utf-8\", errors='replace') as newFile: \n",
    "            data.append(newFile.read())\n",
    "            \n",
    "    for ind, fileData in enumerate(data):\n",
    "        filt = [d == '\\x0c' for d in fileData]\n",
    "        if sum(filt) < len(filt):\n",
    "            s = fileData.lower()\n",
    "            s = s.translate(translator)\n",
    "            dataFiltered.append(s)\n",
    "            filesFiltered.append(allFiles[ind])\n",
    "            \n",
    "    return (dataFiltered, filesFiltered)\n",
    "\n",
    "\n",
    "def document_to_words(doc):\n",
    "    doc = doc.split('\\n')\n",
    "    doc_short = []\n",
    "    doc_words = []\n",
    "    doc_words_short = []\n",
    "    for d in doc:\n",
    "        if len(d)>1: doc_short.append(d)\n",
    "    for d in doc_short:\n",
    "        doc_words += d.split(' ')\n",
    "    for d in doc_words:\n",
    "        if len(d)>1: doc_words_short.append(d)\n",
    "    return doc_words_short\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    '''Remove stop words from an array of tokens'''\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    #nltk.download('stopwords')\n",
    "    #stopWords = set(stopwords.words('english'))\n",
    "    stopWords = ['the', 'to', '-', 'pr', 'der', 'is', 'of', 'die', 'in', 'and', 'und', '–', '•', '✔', '●', 'a']\n",
    "    \n",
    "    tokens_filt = []\n",
    "    for gT in tokens:\n",
    "        if gT not in stopWords: tokens_filt.append(gT)\n",
    "            \n",
    "    return tokens_filt\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    '''Extract different token arrays for every single files in data (>>tokens)\n",
    "    and extract ONE single token array for all files (>>globalTokens)'''\n",
    "    tokens = []\n",
    "    globalData = ''\n",
    "    for d in data: \n",
    "        globalData += ' ' + d\n",
    "        tokens.append(nltk.word_tokenize(d))\n",
    "    globalTokens = nltk.word_tokenize(globalData)\n",
    "    return (tokens, globalTokens)\n",
    "\n",
    "def calc_document_tf(tokens):\n",
    "    '''Compute tf's for every file \n",
    "        -> dict: tf[indFile]['word'] = tf \n",
    "    '''\n",
    "    count = Counter(tokens)\n",
    "    totalCount = sum(count.values())\n",
    "    tf_doc = {}\n",
    "    for c in count:\n",
    "        tf_doc[c] = count[c] / totalCount\n",
    "    return tf_doc\n",
    "\n",
    "def calc_idf(term, tokens_filtered):\n",
    "    '''Compute idf's'''\n",
    "    no_of_doc = 0\n",
    "    conn = []\n",
    "    # Count number of documents, in which the term occurs\n",
    "    for ind, t in enumerate(tokens_filtered):\n",
    "        if (term in t): \n",
    "            no_of_doc += 1\n",
    "            conn.append(ind)\n",
    "    if (no_of_doc > 0): return (log(len(tokens_filtered) / no_of_doc), conn)\n",
    "    else: return (None, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Tokenizing...\n",
      "Removing stopwords...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "# Loading the data files\n",
    "rootDir = '/home/flo/Hackdays_Mannheim_2019/github/docs_txt'\n",
    "print(\"Loading data...\")\n",
    "(data, fileDirs) = get_filtered_data(rootDir)\n",
    "print(\"Tokenizing...\")\n",
    "(tokens, globalTokens) = tokenize(data)\n",
    "print(\"Removing stopwords...\")\n",
    "tokens_filtered = []\n",
    "for t in tokens: tokens_filtered.append(remove_stopwords(t))\n",
    "globalTokens_filtered = remove_stopwords(globalTokens)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate tf's\n",
      "Calculate idf's\n",
      "----------|\n",
      "#"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-55dc874e6c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_glob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalTokens_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalTokens_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mIDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_glob\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_glob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mword_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8200e1d1349e>\u001b[0m in \u001b[0;36mcalc_idf\u001b[0;34m(term, tokens_filtered)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Count number of documents, in which the term occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mno_of_doc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "print(\"Calculate tf's\")\n",
    "tf = calc_document_tf(tokens_filtered[0])\n",
    "# Calculate tf's for every word in every file\n",
    "TF = []\n",
    "for t in tokens_filtered:\n",
    "    tf = calc_document_tf(t)\n",
    "    TF.append(tf)\n",
    "\n",
    "print(\"Calculate idf's\", flush=True)\n",
    "print(\"----------|\")\n",
    "IDF = {}\n",
    "for ind, t_glob in enumerate(globalTokens_filtered):\n",
    "    if (ind % int(0.1*len(globalTokens_filtered)) == 0): print(\"#\", flush=True, end='')\n",
    "    IDF[t_glob] = calc_idf(t_glob, tokens_filtered)\n",
    "\n",
    "word_conn = {}\n",
    "for word in IDF:\n",
    "    conn = IDF[word][1]\n",
    "    tfs = [TF[c][word] for c in conn]\n",
    "    idf = IDF[word][0]\n",
    "    word_conn[word] = (conn, tfs, idf)\n",
    "\n",
    "\n",
    "print(\"\\nDONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = '/home/flo/Hackdays_Mannheim_2019/github/data_structure/data.json'\n",
    "\n",
    "import json\n",
    "jsonFile = json.dumps(word_conn)\n",
    "\n",
    "f = open(export_path,\"w\")\n",
    "f.write(jsonFile)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "token2Files = defaultdict(set)\n",
    "filentoken2occ = defaultdict(int)\n",
    "token2occ = defaultdict(int)\n",
    "for i, tokens in enumerate(tokens_filtered):\n",
    "    for t in tokens:\n",
    "        if i not in token2Files[t]:\n",
    "            token2Files[t].add(i)\n",
    "        filentoken2occ[i, t] += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67009"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered_test = set()\n",
    "for t in tokens_filtered:\n",
    "    tokens_filtered_test.update(t)\n",
    "len(tokens_filtered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "910128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalTokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "910118"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalTokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
